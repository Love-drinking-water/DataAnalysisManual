{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。 ⚠️重点！数据的数量和质量比算法重要的多！\n",
    "# 特征工程包括：\n",
    "# 1.特征使用 （关注点都在特征的原数据上）\n",
    "    # 数据的选择：是分析和我们目标最相关的数据都有哪些，这些数据如何获取。\n",
    "    # 数据的可用性判断：是指数据特征是否可以持续输出\n",
    "# 2.特征获取\n",
    "    # 特征来源：即我们需要的特征来自与哪张表或哪个文件\n",
    "    # 特征存储：如特征来自于不同的文件或不同的数据库，就要将数据进行规整，存储在将来想要使用的媒介中\n",
    "# 3.特征处理：目的是数据属性和特征尽可能大的发挥作用，体现差别\n",
    "    # 数据清洗：\n",
    "    # 特征预处理：\n",
    "# 4.特征监控\n",
    "    # 现有特征：主要看是否对现有的任务还存在积极的作用\n",
    "    # 新的特征：看知否有助于提高效果，或更能代表我们的任务目标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据样本抽样\n",
    "# 样本要具有代表性\n",
    "# 样本比例平衡以及样本不平衡时如何处理\n",
    "# 考虑全量数据 （hadoop、spark大数据工具）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 异常值（空值）处理\n",
    "# 识别异常值和重复值  pandas:isnull()/duplicated()\n",
    "# 直接丢弃（包括重复数据） pandas:drop()/dropna()/drop_duplicated()\n",
    "# 将异常值当作一个新的属性，替代原值 pandas:fillna()\n",
    "# 集中值指代：pandas:fillna() 集中值可以是：除异常值外的均值、中位数、众数等\n",
    "# 边界值指代：pandas:fillna() 利用四分位数的边界\n",
    "# 插值： pandas:interpolate() --- Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'isnull'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-dca3294e873c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 查看空值\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# 删除空值\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'isnull'"
     ]
    }
   ],
   "source": [
    "# 字符类型的空值是：None\n",
    "# 数值类型的空值是：NaN\n",
    "df = [[],[]]\n",
    "# 查看空值\n",
    "df.isnull()\n",
    "# 删除空值\n",
    "df.dropna()\n",
    "# 去掉B行的空值\n",
    "df.dropna(subset=[\"B\"])\n",
    "# 识别重复值\n",
    "df.duplicated([\"A\"])\n",
    "# 删除重复值\n",
    "# keep=\"first\" last, False:删除所有重复值\n",
    "# inplace=True  可以让DF的index也跟着发生变化\n",
    "df.drop_duplicates([\"A\"], keep=\"first\")\n",
    "# 标注异常值\n",
    "df.fillna(\"b*\")\n",
    "# 集中值指代 均值\n",
    "df.fillna(df[\"E\"].mean())\n",
    "# 插值 （只能用于series数据），返回的是相邻两个值的平均值，处于两端的话，就取最近的数\n",
    "# method=\"spline\", order=3  三次样条\n",
    "df[\"E\"].interpolate()\n",
    "# 用四分位数确定上下界的方法进行过滤\n",
    "upper_q = df[\"D\"].quantile(0.75)\n",
    "lower_q = df[\"D\"].quantile(0.25)\n",
    "q_int = upper_q - lower_q\n",
    "k = 1.5\n",
    "df[df[\"D\"]>lower_q-k*q_int][df[\"D\"]<upper_q+k*q_int]\n",
    "# 去除异常值\n",
    "df.drop(2)\n",
    "# 或者\n",
    "df[[True if item.startswith(\"f\") else False for item in list(df[\"F\"].values)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标注（标记、标签、label）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 特征预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征选择：剔除与标注不相关或者冗余的特征，减少特征的个数。作用：减少了训练的时间，减少了过拟合。是数据规约的一种处理方式（另一个为抽样）\n",
    "    # 特征选择有三个切入思路；\n",
    "    # 1.过滤思想：就是直接评价某个特征与标注的相关性的特征，如果与标注的相关性非常小就去掉。（复习下面的特征选择表）\n",
    "    # 2.包裹思想：遍历特征子集。假设所有的集合是一个集合X，最佳的特征组合是它的一个子集，我们的任务就是要找到这个子集。\n",
    "    # RFE算法：\n",
    "    # 第一步：列出集合X。\n",
    "    # 第二步：构造简单的模型进行训练，根据系数去掉比较弱的特征。\n",
    "    # 第三部：余下的特征重复这个过程，直到评价指标下降较大或者低于阀值，停止\n",
    "    # 3.嵌入思想：建立简单的回归模型。根据一个简单的模型来分析特征的重要性。最常用的方式是用正则化的方式来做特征选择。\n",
    "# 特征变换：对指化、离散化、数据平滑、归一化（标准化）、数值化、正规化\n",
    "# 特征降维 \n",
    "# 特征衍生 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 特征选择表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    数据类型                           可用方法\n",
    "# 连续----连续                       相关系数、假设检验\n",
    "# 连续----离散（二值）                相关系数、连续二值化（最小Gini切分，最大熵增益切分）\n",
    "# 连续----离散（非二值）              相关系数（定序）\n",
    "# 离散（二值）----离散（二值）         相关系数、熵相关、F分值\n",
    "# 离散----离散（非二值）              熵相关、Gini、相关系数（定序）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成一组数据\n",
    "df = pd.DataFrame({\"A\":ss.norm.rvs(size=10),\n",
    "               \"B\":ss.norm.rvs(size=10),\n",
    "               \"C\":ss.norm.rvs(size=10),\n",
    "               \"D\":np.random.randint(low=0, high=2, size=10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.068055</td>\n",
       "      <td>-0.443143</td>\n",
       "      <td>1.108843</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.660826</td>\n",
       "      <td>-0.154182</td>\n",
       "      <td>-1.728332</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.075818</td>\n",
       "      <td>-0.223309</td>\n",
       "      <td>0.407641</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-1.089937</td>\n",
       "      <td>-0.967766</td>\n",
       "      <td>-1.140794</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-1.022013</td>\n",
       "      <td>0.151264</td>\n",
       "      <td>-0.160691</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.086538</td>\n",
       "      <td>0.258648</td>\n",
       "      <td>-1.856277</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>-1.424559</td>\n",
       "      <td>-0.083167</td>\n",
       "      <td>1.027530</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>-0.627077</td>\n",
       "      <td>1.153571</td>\n",
       "      <td>-1.524347</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>-0.850279</td>\n",
       "      <td>-0.743631</td>\n",
       "      <td>0.424328</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>-0.427147</td>\n",
       "      <td>-1.664223</td>\n",
       "      <td>1.348109</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A         B         C  D\n",
       "0  0.068055 -0.443143  1.108843  1\n",
       "1  1.660826 -0.154182 -1.728332  1\n",
       "2  0.075818 -0.223309  0.407641  1\n",
       "3 -1.089937 -0.967766 -1.140794  1\n",
       "4 -1.022013  0.151264 -0.160691  1\n",
       "5  0.086538  0.258648 -1.856277  0\n",
       "6 -1.424559 -0.083167  1.027530  1\n",
       "7 -0.627077  1.153571 -1.524347  0\n",
       "8 -0.850279 -0.743631  0.424328  0\n",
       "9 -0.427147 -1.664223  1.348109  1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入SVR回归器、\n",
    "# DecisionTreeRegressor决策树回归器\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确定特征和标注\n",
    "# 特征\n",
    "X = df.loc[:,[\"A\", \"B\", \"C\"]]\n",
    "# 标注\n",
    "Y = df.loc[:, \"D\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征选择常用的三个类\n",
    "# 过滤思想 SelectKBest\n",
    "# 包裹思想 RFE\n",
    "# 嵌入思想 SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, RFE, SelectFromModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 过滤思想"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.44314267,  1.10884264],\n",
       "       [-0.15418172, -1.72833239],\n",
       "       [-0.22330872,  0.40764105],\n",
       "       [-0.96776614, -1.14079445],\n",
       "       [ 0.15126358, -0.16069073],\n",
       "       [ 0.25864802, -1.85627674],\n",
       "       [-0.08316678,  1.02752952],\n",
       "       [ 1.15357136, -1.52434747],\n",
       "       [-0.74363087,  0.424328  ],\n",
       "       [-1.66422293,  1.3481086 ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f_classif 通过方差分析的F值进行判定的（默认的）\n",
    "# mutual_info_classif 互信息\n",
    "# chi2 卡方校验\n",
    "skb = SelectKBest(k=2)\n",
    "skb.fit(X,Y)\n",
    "skb.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 包裹思想 RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06805487, -0.44314267],\n",
       "       [ 1.66082604, -0.15418172],\n",
       "       [ 0.07581797, -0.22330872],\n",
       "       [-1.08993669, -0.96776614],\n",
       "       [-1.02201321,  0.15126358],\n",
       "       [ 0.08653789,  0.25864802],\n",
       "       [-1.42455855, -0.08316678],\n",
       "       [-0.62707671,  1.15357136],\n",
       "       [-0.85027892, -0.74363087],\n",
       "       [-0.42714716, -1.66422293]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVR 线性回归器 \n",
    "# n_features_to_select 最终要选择的特征数\n",
    "# step 每一步要去掉多少个特征  step=1每迭代一次去掉一个特征 \n",
    "rfe = RFE(estimator=SVR(kernel=\"linear\"), n_features_to_select=2, step=1)\n",
    "# fit_transform 拟合过后再进行变换\n",
    "rfe.fit_transform(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 嵌入思想 SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.44314267],\n",
       "       [-0.15418172],\n",
       "       [-0.22330872],\n",
       "       [-0.96776614],\n",
       "       [ 0.15126358],\n",
       "       [ 0.25864802],\n",
       "       [-0.08316678],\n",
       "       [ 1.15357136],\n",
       "       [-0.74363087],\n",
       "       [-1.66422293]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# threshold 表示重要性因子的数（低于多少就会被去掉）\n",
    "sfm = SelectFromModel(estimator=DecisionTreeRegressor(), threshold=0.1)\n",
    "sfm.fit_transform(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征变换：就是根据特征的特性进行一定方式的转换，使特征能够发挥出它的特点\n",
    "# 特征变换的方法：\n",
    "# 1.对指化：对数据进行对数化和指数化的过程  使用函数 softmax 即可完成  numpy.exp\n",
    "# 2.对数化：对数据取对数的过程，底可以取e、10、2  例如月收入情况  numpy.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 离散化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将连续变量分成几段（bins）,变成离散数据的选择。\n",
    "# 原因：\n",
    "# 1.克服数据缺陷\n",
    "# 2.某些算法要求\n",
    "# 3.非线数据映射\n",
    "# 方法：\n",
    "# 数据分箱技术有：等频（等深分箱）、等距（等宽分箱）\n",
    "# 自因变量优化：根据自变量、因变量的有序分布，找到拐点等特殊变化点进行离散化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 离散化（分箱）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据在进行分箱前一定要先进行排序\n",
    "# 深度：数据的个数\n",
    "# 宽度：数据的区间 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "lst = [6, 8, 10, 15, 16, 24, 25, 40, 67]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[low, low, low, medium, medium, medium, high, high, high]\n",
       "Categories (3, object): [low < medium < high]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 等深分箱\n",
    "# qcut 方法\n",
    "# q 分成的份数\n",
    "# labels 标注\n",
    "pd.qcut(lst, q=3, labels=[\"low\", \"medium\", \"high\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5.939, 26.333], (5.939, 26.333], (5.939, 26.333], (5.939, 26.333], (5.939, 26.333], (5.939, 26.333], (5.939, 26.333], (26.333, 46.667], (46.667, 67.0]]\n",
       "Categories (3, interval[float64]): [(5.939, 26.333] < (26.333, 46.667] < (46.667, 67.0]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 等宽分箱\n",
    "# bins=3 分成3段\n",
    "pd.cut(lst, bins=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[low, low, low, low, low, low, low, medium, high]\n",
       "Categories (3, object): [low < medium < high]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.cut(lst, bins=3, labels=[\"low\", \"medium\", \"high\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 归一化和标准化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最大化、最小化的一种特殊形式。归一化将数据转化到 0-1 之间的范围。方法将每个数减去数据的最小值，然后除以最大值与最小值的差。\n",
    "# 好处：处理起来会更方便一些，一方面可以直接观察单个数据相对于整体数据的比例，另一方面如果遇到不同样的数据特征，可以方便的建立起这些数据特征之间合适的距离度量方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据转换成一个标准的形式。\n",
    "# 狭义的理解：将数据缩放到均值为 0， 标准差为 1 的尺度上，这种转化也叫做韦德分标准化（Z-score）。\n",
    "# 方法：一个特征的每个数减去它的均值，然后再除以它的标准差，就得到 Z-score 转化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入归一化与标准化函数 (需要链接外网)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  ],\n",
       "       [0.15],\n",
       "       [0.45],\n",
       "       [0.7 ],\n",
       "       [1.  ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 归一化\n",
    "# reshape(-1, 1)  -1代表不指定有多少行， 1代表必须有一列\n",
    "MinMaxScaler().fit_transform(np.array([1, 4, 10, 15, 21]).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [-1.],\n",
       "       [-1.],\n",
       "       [-1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 标准化\n",
    "StandardScaler().fit_transform(np.array([1, 1, 1, 1, 0, 0, 0, 0]).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.64575131],\n",
       "       [-0.37796447],\n",
       "       [-0.37796447],\n",
       "       [-0.37796447],\n",
       "       [-0.37796447],\n",
       "       [-0.37796447],\n",
       "       [-0.37796447],\n",
       "       [-0.37796447]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StandardScaler().fit_transform(np.array([1,0, 0, 0, 0, 0, 0, 0]).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数值化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数值化就是把非数值数据转化成数值数据的过程。\n",
    "# 数据分类：\n",
    "# 1.定类：将数据进行数值化，使其可以进行四则运算。独热（one-HotEncode）：将数据特征进行扩维，原来的N维属性，用N维向量数据来表示，这个向量只有一位是1，其它均为0.\n",
    "# eg\n",
    "# red  ->[1, 0, 0, 0]\n",
    "# yellow  ->[0, 1, 0, 0]\n",
    "# blue  ->[0, 0, 1, 0]\n",
    "# green  ->[0, 0, 0, 1]\n",
    "\n",
    "# 2.定序：将数据进行数值化，使其可以进行四则运算。通过标签化(labelEncoder)的方式 （0，1）\n",
    "# 3.定距：可以通过归一化的方式，消除其物理含义，从而具备乘除的能力\n",
    "# 4.定比\n",
    "\n",
    "# 数值化的作用是为了以后建模时使用，不会将其直接相加减"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/Applications/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 标签化(labelEncoder)的方式\n",
    "LabelEncoder().fit_transform(np.array([\"Down\", \"Up\", \"Up\", \"Down\"]).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 独热（one-HotEncode）\n",
    "# 要先进行标签化\n",
    "lb_encoder = LabelEncoder()\n",
    "lb_tran_f = lb_encoder.fit_transform(np.array([\"Red\", \"Yellow\", \"Blue\", \"Green\"]))\n",
    "oht_encoder = OneHotEncoder().fit(lb_tran_f.reshape(-1, 1))\n",
    "oht_encoder.transform(lb_encoder.transform(np.array([\"Yellow\", \"Blue\", \"Green\", \"Green\", \"Red\"])).reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正规化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 是将一个向量的长度正规到单位1，如果距离尺度的衡量用L1距离，那就是L1正规化。如果用L2表示（欧式长度），就是L2正则化\n",
    "# 方法：分子保持向量的分量不变，分母用的是各个向量绝对值的和。\n",
    "# 用法：\n",
    "# 1.直接用在特征上。如果正规化用在某一个特征上，可以将特征间的差距转化成考虑相对整体特征长度的一个相对值\n",
    "# 2.用在每个对象的各个特征的表示（特征矩阵的行）。如果每个向量的分量都是一个特征，此时进行正规化，我们可以体现出一个对象特征之间影响的相对关系特点\n",
    "# 3.模型的参数上（回归模型使用较多）。如线性回归、逻辑回归等。比如我们可以用L2正则化让所有的系数的平方和唯一，可以表示出哪些特征对于标注的影响占比比较大，哪些占比比较小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.125,  0.125,  0.375, -0.125,  0.25 ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "# norm=\"l1\" 选择方法 L1、L2\n",
    "Normalizer(norm=\"l1\").fit_transform(np.array([[1, 1, 3, -1, 2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA、奇异值分解等线性降维（没有用到标注）\n",
    "# LDA降维（使用了标注）Linear Discriminant Analysis 线性判别式分析\n",
    "# ⚠️不是：隐含狄利克雷分布（Latent Dirichlet Allocation）,主要用于自然语言处理中主题模型的建立。\n",
    "# 核心思想：投影变换后同一标注内的距离尽可能小；不同标注间的距离尽可能大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.73205081],\n",
       "       [-1.73205081],\n",
       "       [-3.46410162],\n",
       "       [ 1.73205081],\n",
       "       [ 1.73205081],\n",
       "       [ 3.46410162]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solver下的参数   \n",
    "# svd 奇异值分解\n",
    "# lsqr \n",
    "# eigen 特征值\n",
    "\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "Y = np.array([1, 1, 1, 2, 2, 2])\n",
    "LinearDiscriminantAnalysis(n_components=1).fit_transform(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 当作判别器使用 fisher classifi分类器\n",
    "clf = LinearDiscriminantAnalysis(n_components=1).fit(X, Y)\n",
    "clf.predict([[0.8, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -1],\n",
       "       [-2, -1],\n",
       "       [-3, -2],\n",
       "       [ 1,  1],\n",
       "       [ 2,  1],\n",
       "       [ 3,  2]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征衍生"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 是现有的特征进行某些组合生成新的具有含义的特征\n",
    "# 常用的方法有：\n",
    "# 加减乘除\n",
    "# 求导与高阶求导\n",
    "# 人工归纳"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
