{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN \n",
    "# 朴素贝叶斯\n",
    "# 决策树\n",
    "# 支持向量机\n",
    "# 集成方法\n",
    "# 罗吉斯特映射（以回归为主，可做分类，可做回归）\n",
    "# 人工神经网络（以回归为主，可做分类，可做回归）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 距离\n",
    "# 欧式距离：两个点的直线距离\n",
    "# 曼哈顿距离：各个维度的长度差的绝对值的和（横坐标的长度差加上纵坐标的长度差）\n",
    "# 闵可夫斯基距离：各个维度的长度差的绝对值p次方的和，开p次根号\n",
    "# KD-Tree\n",
    "# 如果一个空间中有很多的点，那么如何找到我们随机指定的一个点的附近的K个点。\n",
    "# 通过一个维度将所有的点分成两个部分，两部分的数量尽可能的保持一致。\n",
    "# 然后拿其中的一部分再进行另一个维度上的切分，一次类推，直到不能切分了为止\n",
    "# 这样的话，在空间中就建立了许多大小不一的格子，每个点通过格子的线进行链接（线当作是中间节点，点当作叶子节点），构成了一个树形结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN （K-Nearest Neighbors）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 算法思想\n",
    "# 一个数据集都会有它的标注，那么如果找到了一个点，它k个最近的邻居，一种标注大于了另外一种标注，那么我们就认为，这个点更倾向于与多数点是一致的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "ACC: 0.9742193577064118\n",
      "REC: 0.959794296400187\n",
      "F-Score: 0.9465191332411249\n",
      "Validation\n",
      "ACC: 0.9546666666666667\n",
      "REC: 0.9364406779661016\n",
      "F-Score: 0.9069767441860463\n",
      "Test\n",
      "ACC_test: 0.959\n",
      "REC_test: 0.9281767955801105\n",
      "F-Score_test: 0.9161554192229038\n",
      "Test_2\n",
      "ACC_test: 0.9703333333333334\n",
      "REC_test: 0.9502762430939227\n",
      "F-Score_test: 0.9392491467576791\n"
     ]
    }
   ],
   "source": [
    "# 归一化将数据转化到 0-1 之间的范围\n",
    "# sl:satisfaction_level\n",
    "# le:last_evaluation\n",
    "# npr:number_project\n",
    "# amh:average_monthly_hours\n",
    "# tsc:time_spend_company\n",
    "# wa:Work_accident\n",
    "# pl5:promotion_last_5years\n",
    "# False:MinMaxScaler\n",
    "# True:StandardScaler\n",
    "# 将离散值数值化\n",
    "# dp:department \n",
    "# slr:salary\n",
    "# False:LabelEncoding\n",
    "# True:OneHotEncoder\n",
    "# 降维\n",
    "# lower_d = False  不降维\n",
    "# ld_n = 1 下降的维度\n",
    "def hr_preprocessing(sl=False, le=False, npr=False, amh=False, tsc=False, wa=False, pl5=False, dp=False, slr=False, lower_d = False, ld_n = 1):\n",
    "    df = pd.read_csv('./data/HR.csv')\n",
    "    # 1.清洗数据\n",
    "    df = df.dropna(subset=['satisfaction_level', 'last_evaluation'])\n",
    "    df = df[df['satisfaction_level']<=1][df['salary']!='nme']\n",
    "    # 2.得到标注\n",
    "    label = df[\"left\"]\n",
    "    df = df.drop('left', axis=1)\n",
    "    # 3.特征选择\n",
    "    \n",
    "    # 4.特征处理\n",
    "    scaler_lst = [sl, le, npr, amh, tsc, wa, pl5]\n",
    "    column_lst = ['satisfaction_level', 'last_evaluation', 'number_project', 'average_monthly_hours', 'time_spend_company', 'Work_accident', 'promotion_last_5years']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "        else:\n",
    "            df[column_lst[i]]=StandardScaler().fit_transform(df[column_lst[i]].values.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "    # # 将离散值数值化\n",
    "    scaler_lst = [dp, slr]\n",
    "    column_lst = [\"department\", \"salary\"]\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            if column_lst[i] == \"salary\":\n",
    "                df[column_lst[i]] = [map_salary(s) for s in df['salary'].values]\n",
    "            else:\n",
    "                df[column_lst[i]] = LabelEncoder().fit_transform(df[column_lst[i]])\n",
    "            df[column_lst[i]] = MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "        else:\n",
    "            # OneHotEncoder\n",
    "            df = pd.get_dummies(df, columns=[column_lst[i]])\n",
    "    # 降维\n",
    "    if lower_d:\n",
    "        # n_components 不能大于类的个数\n",
    "        # return LinearDiscriminantAnalysis(n_components=ld_n)\n",
    "        \n",
    "        # PCA n_components 不受限制\n",
    "        return PCA(n_components=ld_n).fit_transform(df.values)\n",
    "        \n",
    "    \n",
    "    return df, label\n",
    "\n",
    "d = dict([('low', 0), ('medium', 1), ('high', 2)])\n",
    "def map_salary(s):\n",
    "    return d.get(s, 0)\n",
    "\n",
    "# 训练集 验证集 测试集\n",
    "def hr_modeling(features, label):\n",
    "    # 引入切分训练集和测试集的函数\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    # 先取值\n",
    "    f_v = features.values\n",
    "    l_v = label.values\n",
    "    # test_size 测试集占多少比例  \n",
    "    # X_tt  训练集部分 X_validation标注\n",
    "    # y_tt  验证集部分 Y_validation标注\n",
    "    # 得到验证集\n",
    "    X_tt, X_validation, Y_tt, Y_validation = train_test_split(f_v, l_v, test_size=0.2)\n",
    "    # 区分验证集和测试集\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_tt, Y_tt, test_size=0.25)\n",
    "    # print(len(X_train), len(X_validation), len(X_test))\n",
    "    \n",
    "    # 衡量指标\n",
    "    # accuracy_score 准确率\n",
    "    # recall_score 召回率\n",
    "    # f1_score F值\n",
    "    from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "    # KNN\n",
    "    # NearestNeighbors 此方法可以直接获得一个点附近最近的几个点\n",
    "    from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\n",
    "    models = []\n",
    "    # n_neigbors 获取最近的几个点\n",
    "    # radius 半径\n",
    "    # algorithm:{'auto', 'ball_tree', 'kd_tree', 'brute'} 指定索引方法\n",
    "    # kd_tree 针对方格子进行的索引  ball_tree 针对球体进行的索引  brute 遍历\n",
    "    # p 就是闵可夫斯基距离公式中的 p 默认为2 \n",
    "    # n_jobs 并行计算的单元数量\n",
    "    knn_clf = KNeighborsClassifier(n_neighbors=3)\n",
    "    knn_clf.fit(X_train, Y_train)\n",
    "        \n",
    "    # 利用训练集进行预测\n",
    "    print(\"Train\")\n",
    "    Y_pred = knn_clf.predict(X_train)\n",
    "    print(\"ACC:\", accuracy_score(Y_train, Y_pred))\n",
    "    print(\"REC:\", recall_score(Y_train, Y_pred))\n",
    "    print(\"F-Score:\", f1_score(Y_train, Y_pred))\n",
    "    \n",
    "    print(\"Validation\")\n",
    "    # 预测 验证集 得到一个预测值\n",
    "    Y_pred = knn_clf.predict(X_validation)\n",
    "    # 拿预测值与实际值进行比较\n",
    "    print(\"ACC:\", accuracy_score(Y_validation, Y_pred))\n",
    "    print(\"REC:\", recall_score(Y_validation, Y_pred))\n",
    "    print(\"F-Score:\", f1_score(Y_validation, Y_pred))\n",
    "    \n",
    "    # 利用测试集进行对比\n",
    "    print(\"Test\")\n",
    "    Y_pred = knn_clf.predict(X_test)\n",
    "    print(\"ACC_test:\", accuracy_score(Y_test, Y_pred))\n",
    "    print(\"REC_test:\", recall_score(Y_test, Y_pred))\n",
    "    print(\"F-Score_test:\", f1_score(Y_test, Y_pred))\n",
    "    \n",
    "    # 将模型进行存储\n",
    "    from sklearn.externals import joblib\n",
    "    # 保存模型\n",
    "    # joblib.dump(knn_clf, \"knn_clf\")\n",
    "    # 使用模型\n",
    "    knn_clf = joblib.load(\"knn_clf\")\n",
    "    print(\"Test_2\")\n",
    "    Y_pred = knn_clf.predict(X_test)\n",
    "    print(\"ACC_test:\", accuracy_score(Y_test, Y_pred))\n",
    "    print(\"REC_test:\", recall_score(Y_test, Y_pred))\n",
    "    print(\"F-Score_test:\", f1_score(Y_test, Y_pred))\n",
    "    \n",
    "def main():\n",
    "    # print(hr_preprocessing(lower_d = False, ld_n = 3))\n",
    "    features, label = hr_preprocessing()\n",
    "    hr_modeling(features, label)\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 朴素贝叶斯 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 概率：P(A)\n",
    "# 条件概率：P(A|B) 条件B发生的条件下，条件A发生的概率\n",
    "# 联合概率：P(A, B) 条件A,B同时发生的概率 \n",
    "# P(A, B) = P(A|B)P(B) = P(B|A)P(A)\n",
    "# 全概率公式：B的概率等于：A事件发生的各种情况下，B事件发生的概率的和\n",
    "# 拉普拉斯平滑：若存在0值，就将所有的数据都加上1，可以规避样本数量等于0的情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "KNN -ACC: 0.974886098455384\n",
      "KNN -REC: 0.9619489559164733\n",
      "KNN -F-Score: 0.9483074107959744\n",
      "1\n",
      "KNN -ACC: 0.9543333333333334\n",
      "KNN -REC: 0.9188034188034188\n",
      "KNN -F-Score: 0.9039943938332164\n",
      "2\n",
      "KNN -ACC: 0.95\n",
      "KNN -REC: 0.927170868347339\n",
      "KNN -F-Score: 0.8982360922659431\n",
      "0\n",
      "GaussianNB: -ACC: 0.796199577730859\n",
      "GaussianNB: -REC: 0.7461716937354989\n",
      "GaussianNB: -F-Score: 0.6368316831683168\n",
      "1\n",
      "GaussianNB: -ACC: 0.7953333333333333\n",
      "GaussianNB: -REC: 0.7393162393162394\n",
      "GaussianNB: -F-Score: 0.6283292978208234\n",
      "2\n",
      "GaussianNB: -ACC: 0.8003333333333333\n",
      "GaussianNB: -REC: 0.7310924369747899\n",
      "GaussianNB: -F-Score: 0.6354230066950699\n",
      "0\n",
      "BernoulliNB: -ACC: 0.8413157017446383\n",
      "BernoulliNB: -REC: 0.46728538283062643\n",
      "BernoulliNB: -F-Score: 0.5851249273678093\n",
      "1\n",
      "BernoulliNB: -ACC: 0.845\n",
      "BernoulliNB: -REC: 0.47150997150997154\n",
      "BernoulliNB: -F-Score: 0.5874001774622893\n",
      "2\n",
      "BernoulliNB: -ACC: 0.8386666666666667\n",
      "BernoulliNB: -REC: 0.469187675070028\n",
      "BernoulliNB: -F-Score: 0.5805892547660312\n"
     ]
    }
   ],
   "source": [
    "# 归一化将数据转化到 0-1 之间的范围\n",
    "# sl:satisfaction_level\n",
    "# le:last_evaluation\n",
    "# npr:number_project\n",
    "# amh:average_monthly_hours\n",
    "# tsc:time_spend_company\n",
    "# wa:Work_accident\n",
    "# pl5:promotion_last_5years\n",
    "# False:MinMaxScaler\n",
    "# True:StandardScaler\n",
    "# 将离散值数值化\n",
    "# dp:department \n",
    "# slr:salary\n",
    "# False:LabelEncoding\n",
    "# True:OneHotEncoder\n",
    "# 降维\n",
    "# lower_d = False  不降维\n",
    "# ld_n = 1 下降的维度\n",
    "def hr_preprocessing(sl=False, le=False, npr=False, amh=False, tsc=False, wa=False, pl5=False, dp=False, slr=False, lower_d = False, ld_n = 1):\n",
    "    df = pd.read_csv('./data/HR.csv')\n",
    "    # 1.清洗数据\n",
    "    df = df.dropna(subset=['satisfaction_level', 'last_evaluation'])\n",
    "    df = df[df['satisfaction_level']<=1][df['salary']!='nme']\n",
    "    # 2.得到标注\n",
    "    label = df[\"left\"]\n",
    "    df = df.drop('left', axis=1)\n",
    "    # 3.特征选择\n",
    "    \n",
    "    # 4.特征处理\n",
    "    scaler_lst = [sl, le, npr, amh, tsc, wa, pl5]\n",
    "    column_lst = ['satisfaction_level', 'last_evaluation', 'number_project', 'average_monthly_hours', 'time_spend_company', 'Work_accident', 'promotion_last_5years']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "        else:\n",
    "            df[column_lst[i]]=StandardScaler().fit_transform(df[column_lst[i]].values.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "    # # 将离散值数值化\n",
    "    scaler_lst = [dp, slr]\n",
    "    column_lst = [\"department\", \"salary\"]\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            if column_lst[i] == \"salary\":\n",
    "                df[column_lst[i]] = [map_salary(s) for s in df['salary'].values]\n",
    "            else:\n",
    "                df[column_lst[i]] = LabelEncoder().fit_transform(df[column_lst[i]])\n",
    "            df[column_lst[i]] = MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "        else:\n",
    "            # OneHotEncoder\n",
    "            df = pd.get_dummies(df, columns=[column_lst[i]])\n",
    "    # 降维\n",
    "    if lower_d:\n",
    "        # n_components 不能大于类的个数\n",
    "        # return LinearDiscriminantAnalysis(n_components=ld_n)\n",
    "        \n",
    "        # PCA n_components 不受限制\n",
    "        return PCA(n_components=ld_n).fit_transform(df.values)\n",
    "        \n",
    "    \n",
    "    return df, label\n",
    "\n",
    "d = dict([('low', 0), ('medium', 1), ('high', 2)])\n",
    "def map_salary(s):\n",
    "    return d.get(s, 0)\n",
    "\n",
    "# 训练集 验证集 测试集\n",
    "def hr_modeling(features, label):\n",
    "    # 引入切分训练集和测试集的函数\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    # 先取值\n",
    "    f_v = features.values\n",
    "    l_v = label.values\n",
    "    # test_size 测试集占多少比例  \n",
    "    # X_tt  训练集部分 X_validation标注\n",
    "    # y_tt  验证集部分 Y_validation标注\n",
    "    # 得到验证集\n",
    "    X_tt, X_validation, Y_tt, Y_validation = train_test_split(f_v, l_v, test_size=0.2)\n",
    "    # 区分验证集和测试集\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_tt, Y_tt, test_size=0.25)\n",
    "    # print(len(X_train), len(X_validation), len(X_test))\n",
    "    \n",
    "    # 衡量指标\n",
    "    # accuracy_score 准确率\n",
    "    # recall_score 召回率\n",
    "    # f1_score F值\n",
    "    from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "    # KNN\n",
    "    # NearestNeighbors 此方法可以直接获得一个点附近最近的几个点\n",
    "    from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\n",
    "    # 朴素贝叶斯 （适用与离散型的数值）\n",
    "    # GaussianNB 高斯朴素贝叶斯 （特征是高斯分布的）\n",
    "    # BernoulliNB伯努利朴素贝叶斯 （数值是离散的，二值，0和1）若是连续值，也需要先将数值离散化\n",
    "    from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "    models = []\n",
    "    # n_neigbors 获取最近的几个点\n",
    "    # radius 半径\n",
    "    # algorithm:{'auto', 'ball_tree', 'kd_tree', 'brute'} 指定索引方法\n",
    "    # kd_tree 针对方格子进行的索引  ball_tree 针对球体进行的索引  brute 遍历\n",
    "    # p 就是闵可夫斯基距离公式中的 p 默认为2 \n",
    "    # n_jobs 并行计算的单元数量\n",
    "    models.append((\"KNN\", KNeighborsClassifier(n_neighbors=3)))\n",
    "    # 高斯贝叶斯\n",
    "    models.append((\"GaussianNB:\", GaussianNB()))\n",
    "    # 伯努利朴素贝叶斯\n",
    "    models.append((\"BernoulliNB:\", BernoulliNB()))\n",
    "    \n",
    "    for clf_name, clf in models:\n",
    "        # 先进行拟合 训练\n",
    "        clf.fit(X_train, Y_train)\n",
    "        # 进行预测和评价 【训练集，验证集，测试集】\n",
    "        xy_lst = [(X_train, Y_train), (X_validation, Y_validation), (X_test, Y_test)]\n",
    "        for i in range(len(xy_lst)):\n",
    "            # X_part\n",
    "            X_part = xy_lst[i][0]\n",
    "            # y_part\n",
    "            Y_part = xy_lst[i][1]\n",
    "            # 预测值\n",
    "            Y_pred = clf.predict(X_part)\n",
    "            # 0训练集 1验证集 2测试集\n",
    "            print(i)\n",
    "            print(clf_name, \"-ACC:\", accuracy_score(Y_part,  Y_pred))\n",
    "            print(clf_name, \"-REC:\", recall_score(Y_part,  Y_pred))\n",
    "            print(clf_name, \"-F-Score:\", f1_score(Y_part,  Y_pred))\n",
    "    \n",
    "def main():\n",
    "    # print(hr_preprocessing(lower_d = False, ld_n = 3))\n",
    "    features, label = hr_preprocessing()\n",
    "    hr_modeling(features, label)\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成模型与判别模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成模型（对数据要求高，速度快一些）：通过求输入与输出的联合概率分布，再求解类别归类的概率。比如朴素贝叶斯模型（因为其分子是一个联合概率分布）\n",
    "# 判别模型（数据要求不高，速度慢一些，使用的范围更广）：不通过联合概率分布，直接就可以获得输出对应最大分类的概率。比如KNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
