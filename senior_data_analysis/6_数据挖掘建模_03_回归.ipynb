{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 线性回归\n",
    "## 决策树回归\n",
    "## 支持向量机回归\n",
    "## 集成方法回归\n",
    "# 罗吉斯特映射回归\n",
    "# 人工神经网络回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回归分析是确定多个变量间相互依赖的定量关系的一种统计分析方法。\n",
    "# 如果多个变量间的关系用线性关系去考量，那就是线性回归。如果用多项式关系去考量，那就是多项式回归。\n",
    "# 最小二乘法\n",
    "# 核心的算法：是一个最优化的过程，是求一个函数最小值的过程，而且也可以证明这个函数是连续可导的。方法：对函数的未知数进行求导，求各个极值，如果值域不是负无穷，那么极小值中一定会有一个最小的。\n",
    "# 梯度下降法：求最小值，节省计算机资源开销。导数：标量（也就是斜率的大小）  梯度：矢量（指定了各个方向上的导数的大小，同时梯度要求函数具有一阶偏导，也是说对每个参数的分量都要可导），梯度指的方向是一个点最大的上升方向。\n",
    "# 线性回归的进化（正则化）。选择系数更小的，可以有效的防止过拟合，也可以更快速的找到最优点\n",
    "    # 岭回归：用到的是L2正则化\n",
    "    # Lasso回归："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "LogisticRegression -ACC: 0.7909767751972442\n",
      "LogisticRegression -REC: 0.3385269121813031\n",
      "LogisticRegression -F-Score: 0.432579185520362\n",
      "1\n",
      "LogisticRegression -ACC: 0.7956666666666666\n",
      "LogisticRegression -REC: 0.34438040345821325\n",
      "LogisticRegression -F-Score: 0.4381301558203483\n",
      "2\n",
      "LogisticRegression -ACC: 0.777\n",
      "LogisticRegression -REC: 0.3254281949934124\n",
      "LogisticRegression -F-Score: 0.4247635425623388\n",
      "0\n",
      "GBDT -ACC: 0.9929992221357928\n",
      "GBDT -REC: 0.9726156751652503\n",
      "GBDT -F-Score: 0.9849390389672483\n",
      "1\n",
      "GBDT -ACC: 0.9873333333333333\n",
      "GBDT -REC: 0.9610951008645533\n",
      "GBDT -F-Score: 0.9723032069970846\n",
      "2\n",
      "GBDT -ACC: 0.9826666666666667\n",
      "GBDT -REC: 0.9472990777338604\n",
      "GBDT -F-Score: 0.9651006711409396\n"
     ]
    }
   ],
   "source": [
    "# 回归测试\n",
    "def regr_test(features, label):\n",
    "    print(\"X\", features)\n",
    "    print(\"Y\", label)\n",
    "    # LinearRegression 线性回归\n",
    "    # Ridge 岭回归\n",
    "    # Lasso\n",
    "    from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "    # 线性回归 (常用)\n",
    "    # regr = LinearRegression()\n",
    "    # 岭回归 （特殊情况使用）\n",
    "    # regr = Ridge(alpha=0.8)\n",
    "    # Lasso （特殊情况使用）\n",
    "    regr = Lasso(alpha=0.004)\n",
    "    \n",
    "    regr.fit(features.values, label.values)\n",
    "    # 预测\n",
    "    Y_pred = regr.predict(features.values)\n",
    "    # 参数\n",
    "    print(\"Coef:\", regr.coef_)\n",
    "    # 衡量回归方程的好坏\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    print(\"MSE:\", mean_squared_error(Y_pred, label.values))\n",
    "\n",
    "def hr_preprocessing(sl=False, le=False, npr=False, amh=False, tsc=False, wa=False, pl5=False, dp=False, slr=False, lower_d = False, ld_n = 1):\n",
    "    df = pd.read_csv('./data/HR.csv')\n",
    "    # 1.清洗数据\n",
    "    df = df.dropna(subset=['satisfaction_level', 'last_evaluation'])\n",
    "    df = df[df['satisfaction_level']<=1][df['salary']!='nme']\n",
    "    # 2.得到标注\n",
    "    label = df[\"left\"]\n",
    "    df = df.drop('left', axis=1)\n",
    "    # 3.特征选择\n",
    "\n",
    "    # 4.特征处理\n",
    "    scaler_lst = [sl, le, npr, amh, tsc, wa, pl5]\n",
    "    column_lst = ['satisfaction_level', 'last_evaluation', 'number_project', 'average_monthly_hours', 'time_spend_company', 'Work_accident', 'promotion_last_5years']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "        else:\n",
    "            df[column_lst[i]]=StandardScaler().fit_transform(df[column_lst[i]].values.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "    # # 将离散值数值化\n",
    "    scaler_lst = [dp, slr]\n",
    "    column_lst = [\"department\", \"salary\"]\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            if column_lst[i] == \"salary\":\n",
    "                df[column_lst[i]] = [map_salary(s) for s in df['salary'].values]\n",
    "            else:\n",
    "                df[column_lst[i]] = LabelEncoder().fit_transform(df[column_lst[i]])\n",
    "            df[column_lst[i]] = MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "        else:\n",
    "            # OneHotEncoder\n",
    "            df = pd.get_dummies(df, columns=[column_lst[i]])\n",
    "    # 降维\n",
    "    if lower_d:\n",
    "        # n_components 不能大于类的个数\n",
    "        # return LinearDiscriminantAnalysis(n_components=ld_n)\n",
    "        # PCA n_components 不受限制\n",
    "        return PCA(n_components=ld_n).fit_transform(df.values)  \n",
    "    return df, label\n",
    "\n",
    "d = dict([('low', 0), ('medium', 1), ('high', 2)])\n",
    "def map_salary(s):\n",
    "    return d.get(s, 0)\n",
    "\n",
    "# 训练集 验证集 测试集\n",
    "def hr_modeling(features, label):\n",
    "    # 引入切分训练集和测试集的函数\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    # 先取值\n",
    "    f_v = features.values\n",
    "    l_v = label.values\n",
    "    # test_size 测试集占多少比例  \n",
    "    # X_tt  训练集部分 X_validation标注\n",
    "    # y_tt  验证集部分 Y_validation标注\n",
    "    # 得到验证集\n",
    "    X_tt, X_validation, Y_tt, Y_validation = train_test_split(f_v, l_v, test_size=0.2)\n",
    "    # 区分验证集和测试集\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_tt, Y_tt, test_size=0.25)\n",
    "    # print(len(X_train), len(X_validation), len(X_test))\n",
    "    \n",
    "    # 衡量指标\n",
    "    # accuracy_score 准确率\n",
    "    # recall_score 召回率\n",
    "    # f1_score F值\n",
    "    from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "    # KNN\n",
    "    # NearestNeighbors 此方法可以直接获得一个点附近最近的几个点\n",
    "    from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\n",
    "    # 逻辑回归\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    # 提升树 GBDT\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    \n",
    "    \n",
    "#     # 人工神经网络\n",
    "#     # Sequential 层级容器\n",
    "#     from keras.models import Sequential\n",
    "#     # Dense 神经网络层/稠密层， Activation激活函数\n",
    "#     from keras.layers.core import Dense, Activation\n",
    "#     # 随机梯度下降算法\n",
    "#     from keras.optimizers import SGD \n",
    "#     # 反向传播算法 PyBrain\n",
    "#     mdl = Sequential()\n",
    "#     # 创建稠密层表示输入层\n",
    "#     # 50 下个隐含层的神经元个数\n",
    "#     # input_dim  输入的维度\n",
    "#     mdl.add(Dense(50, input_dim=len(f_v[0])))\n",
    "#     # 激活函数\n",
    "#     mdl.add(Activation(\"sigmoid\"))\n",
    "#     # 接入输出层 (上一层输出几个，下一层就会输入几个)\n",
    "#     mdl.add(Dense(2))\n",
    "#     # 保证归一化\n",
    "#     mdl.add(Activation(\"softmax\"))\n",
    "#     # 优化器 lr 学习率\n",
    "#     sgd = SGD(lr=0.08)\n",
    "#     # 编译建立模型\n",
    "#     # loss 最优化函数、损失函数\n",
    "#     # mean_squared_error 平均平方误差\n",
    "#     # optimizer 使用什么样的优化器  sgd  adam(亚当优化器)，迈开步子跑 \n",
    "#     # nb_epoch 一共迭代的次数 \n",
    "#     # batch_size 随机梯度下降算法每次选取的数量。\n",
    "#     mdl.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "#     mdl.fit(X_train, np.array([[0, 1] if i==1 else [1, 0] for i in Y_train]), nb_epoch=10000, batch_size=8999)\n",
    "    \n",
    "#     # 进行预测和评价 【训练集，验证集，测试集】\n",
    "#     xy_lst = [(X_train, Y_train), (X_validation, Y_validation), (X_test, Y_test)]\n",
    "#     for i in range(len(xy_lst)):\n",
    "#         # X_part\n",
    "#         X_part = xy_lst[i][0]\n",
    "#         # y_part\n",
    "#         Y_part = xy_lst[i][1]\n",
    "#         # 预测值\n",
    "#         Y_pred = mdl.predict_classes(X_part)\n",
    "#         # 0训练集 1验证集 2测试集\n",
    "#         print(i)\n",
    "#         print(\"NN\", \"-ACC:\", accuracy_score(Y_part,  Y_pred))\n",
    "#         print(\"NN\", \"-REC:\", recall_score(Y_part,  Y_pred))\n",
    "#         print(\"NN\", \"-F-Score:\", f1_score(Y_part,  Y_pred))\n",
    "#     return \n",
    "\n",
    "    models = []\n",
    "    # Adaboost\n",
    "    # random_state 随机数的种子值\n",
    "    # base_estimator 弱分类的基本分类器 需要两个属性 classes_ 和 n_classes_ 默认的是决策树\n",
    "    # n_estimators 及联的决策树的数量 默认是50个\n",
    "    # learning_rate 根据0-1的数进行衰减\n",
    "    # algorithm 针对 base_estimator 进行的选择，如果base_estimator可以得到一个判别的概率，那么就可以使用SAMME.R,得到更好的效果\n",
    "#     models.append((\"AdaBoost\", AdaBoostClassifier(n_estimators=100)))\n",
    "    # penalty l1,l2 使用L1正则化或L2正则化\n",
    "    # tol 计算的精度\n",
    "    # C C越大，正则化因子所占的比例越小，反之越大\n",
    "    # solver 使用的方法 sag 随机平均梯度下降算法 默认是liblinear\n",
    "    # max_iter 最大的迭代次数\n",
    "    # coef_ 参数\n",
    "    # intercept_ 截距\n",
    "    models.append((\"LogisticRegression\", LogisticRegression(C=1000, tol=1e-10, solver=\"sag\", max_iter=10000)))\n",
    "    # 提升树 GBDT\n",
    "    # max_depth 深度\n",
    "    # n_estimators 树的数量\n",
    "    # learning_rate 衰减比率\n",
    "    # criterion 标准\n",
    "    models.append((\"GBDT\", GradientBoostingClassifier(max_depth=6, n_estimators=100)))\n",
    "    for clf_name, clf in models:\n",
    "        # 先进行拟合 训练\n",
    "        clf.fit(X_train, Y_train)\n",
    "        # 进行预测和评价 【训练集，验证集，测试集】\n",
    "        xy_lst = [(X_train, Y_train), (X_validation, Y_validation), (X_test, Y_test)]\n",
    "        for i in range(len(xy_lst)):\n",
    "            # X_part\n",
    "            X_part = xy_lst[i][0]\n",
    "            # y_part\n",
    "            Y_part = xy_lst[i][1]\n",
    "            # 预测值\n",
    "            Y_pred = clf.predict(X_part)\n",
    "            # 0训练集 1验证集 2测试集\n",
    "            print(i)\n",
    "            print(clf_name, \"-ACC:\", accuracy_score(Y_part,  Y_pred))\n",
    "            print(clf_name, \"-REC:\", recall_score(Y_part,  Y_pred))\n",
    "            print(clf_name, \"-F-Score:\", f1_score(Y_part,  Y_pred))\n",
    "            \n",
    "def main():\n",
    "    # print(hr_preprocessing(lower_d = False, ld_n = 3))\n",
    "    features, label = hr_preprocessing()\n",
    "#     regr_test(features[[\"number_project\", \"average_monthly_hours\"]], features[\"last_evaluation\"])\n",
    "    \n",
    "    hr_modeling(features, label)\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 罗吉（逻辑）斯特回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 逻辑斯特增长模型（逻辑斯蒂增长模型）\n",
    "# Logistic Regression 逻辑回归的值域是有限的\n",
    "# 代码看上面"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 人工神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将多个感知器链接起来，就构成了神经网络\n",
    "# 神经网络的输入层需要将所有的特征归一化（0-1之间的数），直接与标注相连的一层，是输出层，必须是one-hot形式的，也就是说，如果有三个类别，那么每个类别都占一位。中间属于隐含层，可以有一层也可以有多层。中间的转换函数也叫做激活函数。\n",
    "# 反向传播算法：\n",
    "    # 1.前向计算\n",
    "    # 2.计算误差\n",
    "    # 3.反向单层调整：只调整离输出层最近的一层的系数，根据误差进行调整\n",
    "    # 4.传播\n",
    "    # 反复循环直到收敛（达到一定的误差范围之内）\n",
    "\n",
    "# 随机梯度下降法 ：每次调整权值时，选取部分样本进行梯度下降。\n",
    "    # 好处：收敛更快，计算开销少\n",
    "    # 问题：容易陷入局部最优解\n",
    "\n",
    "# 其它问题：\n",
    "    # 1.易受离群点的影响，易过拟合（解决办法：正则化，dropout）\n",
    "    # 2.属性与结果要在0-1之间\n",
    "    # 3.输出结果进行softMax转换\n",
    "\n",
    "# mac 端安装 keras \n",
    "# pip install tensorflow\n",
    "# pip install keras\n",
    "# 代码看上面\n",
    "\n",
    "# 逻辑回归\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# 人工神经网络\n",
    "# Sequential 层级容器\n",
    "from keras.models import Sequential\n",
    "# Dense 神经网络层/稠密层， Activation激活函数\n",
    "from keras.layers.core import Dense, Activation\n",
    "# 随机梯度下降算法\n",
    "from keras.optimizers import SGD \n",
    "# 反向传播算法 PyBrain\n",
    "mdl = Sequential()\n",
    "# 创建稠密层表示输入层\n",
    "# 50 下个隐含层的神经元个数\n",
    "# input_dim  输入的维度\n",
    "mdl.add(Dense(50, input_dim=len(f_v[0])))\n",
    "# 激活函数\n",
    "mdl.add(Activation(\"sigmoid\"))\n",
    "# 接入输出层 (上一层输出几个，下一层就会输入几个)\n",
    "mdl.add(Dense(2))\n",
    "# 保证归一化\n",
    "mdl.add(Activation(\"softmax\"))\n",
    "# 优化器 lr 学习率\n",
    "sgd = SGD(lr=0.08)\n",
    "# 编译建立模型\n",
    "# loss 最优化函数、损失函数\n",
    "# mean_squared_error 平均平方误差\n",
    "# optimizer 使用什么样的优化器  sgd  adam(亚当优化器)，迈开步子跑 \n",
    "# nb_epoch 一共迭代的次数 \n",
    "# batch_size 随机梯度下降算法每次选取的数量。\n",
    "mdl.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "mdl.fit(X_train, np.array([[0, 1] if i==1 else [1, 0] for i in Y_train]), nb_epoch=10000, batch_size=8999)\n",
    "\n",
    "# 进行预测和评价 【训练集，验证集，测试集】\n",
    "xy_lst = [(X_train, Y_train), (X_validation, Y_validation), (X_test, Y_test)]\n",
    "for i in range(len(xy_lst)):\n",
    "    # X_part\n",
    "    X_part = xy_lst[i][0]\n",
    "    # y_part\n",
    "    Y_part = xy_lst[i][1]\n",
    "    # 预测值\n",
    "    Y_pred = mdl.predict_classes(X_part)\n",
    "    # 0训练集 1验证集 2测试集\n",
    "    print(i)\n",
    "    print(\"NN\", \"-ACC:\", accuracy_score(Y_part,  Y_pred))\n",
    "    print(\"NN\", \"-REC:\", recall_score(Y_part,  Y_pred))\n",
    "    print(\"NN\", \"-F-Score:\", f1_score(Y_part,  Y_pred))\n",
    "return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 回归树与提升树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回归树中不但包括叶子节点还包括中间节点都会得到一个预测值，一般来说这些预测值就是这些连续标注的平均值。\n",
    "# 最小方差的切分方法：若根据某一个特征进行切分，一定要满足在切分后，这两个部分的方差的和是最小的。\n",
    "# 停止条件：可以是剪枝的限制，比如说：树的最大深度，每一页的最大样本数量等。也有可能是最小方差的值。\n",
    "# 使用最多的是梯度提升决策树  GBDT (Gradient Boosting Decision Tree)\n",
    "# 直到将 差分值 （上面的数减去下面的数）变得越来越小结束\n",
    "# 类似的算法 Xgboost\n",
    "    # 优点：不仅考虑了一阶导数，还可能会用到更高阶的导数，并且支持较大规模的并行计算，分类器的选择更加灵活。\n",
    "# 代码看上面"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
