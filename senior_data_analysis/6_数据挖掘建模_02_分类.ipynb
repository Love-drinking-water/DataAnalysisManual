{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN \n",
    "# 朴素贝叶斯\n",
    "# 决策树\n",
    "# 支持向量机\n",
    "# 集成方法\n",
    "# 罗吉斯特映射（以回归为主，可做分类，可做回归）\n",
    "# 人工神经网络（以回归为主，可做分类，可做回归）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 距离\n",
    "# 欧式距离：两个点的直线距离\n",
    "# 曼哈顿距离：各个维度的长度差的绝对值的和（横坐标的长度差加上纵坐标的长度差）\n",
    "# 闵可夫斯基距离：各个维度的长度差的绝对值p次方的和，开p次根号\n",
    "# KD-Tree\n",
    "# 如果一个空间中有很多的点，那么如何找到我们随机指定的一个点的附近的K个点。\n",
    "# 通过一个维度将所有的点分成两个部分，两部分的数量尽可能的保持一致。\n",
    "# 然后拿其中的一部分再进行另一个维度上的切分，一次类推，直到不能切分了为止\n",
    "# 这样的话，在空间中就建立了许多大小不一的格子，每个点通过格子的线进行链接（线当作是中间节点，点当作叶子节点），构成了一个树形结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN （K-Nearest Neighbors）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 算法思想\n",
    "# 一个数据集都会有它的标注，那么如果找到了一个点，它k个最近的邻居，一种标注大于了另外一种标注，那么我们就认为，这个点更倾向于与多数点是一致的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "ACC: 0.9742193577064118\n",
      "REC: 0.959794296400187\n",
      "F-Score: 0.9465191332411249\n",
      "Validation\n",
      "ACC: 0.9546666666666667\n",
      "REC: 0.9364406779661016\n",
      "F-Score: 0.9069767441860463\n",
      "Test\n",
      "ACC_test: 0.959\n",
      "REC_test: 0.9281767955801105\n",
      "F-Score_test: 0.9161554192229038\n",
      "Test_2\n",
      "ACC_test: 0.9703333333333334\n",
      "REC_test: 0.9502762430939227\n",
      "F-Score_test: 0.9392491467576791\n"
     ]
    }
   ],
   "source": [
    "# 归一化将数据转化到 0-1 之间的范围\n",
    "# sl:satisfaction_level\n",
    "# le:last_evaluation\n",
    "# npr:number_project\n",
    "# amh:average_monthly_hours\n",
    "# tsc:time_spend_company\n",
    "# wa:Work_accident\n",
    "# pl5:promotion_last_5years\n",
    "# False:MinMaxScaler\n",
    "# True:StandardScaler\n",
    "# 将离散值数值化\n",
    "# dp:department \n",
    "# slr:salary\n",
    "# False:LabelEncoding\n",
    "# True:OneHotEncoder\n",
    "# 降维\n",
    "# lower_d = False  不降维\n",
    "# ld_n = 1 下降的维度\n",
    "def hr_preprocessing(sl=False, le=False, npr=False, amh=False, tsc=False, wa=False, pl5=False, dp=False, slr=False, lower_d = False, ld_n = 1):\n",
    "    df = pd.read_csv('./data/HR.csv')\n",
    "    # 1.清洗数据\n",
    "    df = df.dropna(subset=['satisfaction_level', 'last_evaluation'])\n",
    "    df = df[df['satisfaction_level']<=1][df['salary']!='nme']\n",
    "    # 2.得到标注\n",
    "    label = df[\"left\"]\n",
    "    df = df.drop('left', axis=1)\n",
    "    # 3.特征选择\n",
    "    \n",
    "    # 4.特征处理\n",
    "    scaler_lst = [sl, le, npr, amh, tsc, wa, pl5]\n",
    "    column_lst = ['satisfaction_level', 'last_evaluation', 'number_project', 'average_monthly_hours', 'time_spend_company', 'Work_accident', 'promotion_last_5years']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "        else:\n",
    "            df[column_lst[i]]=StandardScaler().fit_transform(df[column_lst[i]].values.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "    # # 将离散值数值化\n",
    "    scaler_lst = [dp, slr]\n",
    "    column_lst = [\"department\", \"salary\"]\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            if column_lst[i] == \"salary\":\n",
    "                df[column_lst[i]] = [map_salary(s) for s in df['salary'].values]\n",
    "            else:\n",
    "                df[column_lst[i]] = LabelEncoder().fit_transform(df[column_lst[i]])\n",
    "            df[column_lst[i]] = MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "        else:\n",
    "            # OneHotEncoder\n",
    "            df = pd.get_dummies(df, columns=[column_lst[i]])\n",
    "    # 降维\n",
    "    if lower_d:\n",
    "        # n_components 不能大于类的个数\n",
    "        # return LinearDiscriminantAnalysis(n_components=ld_n)\n",
    "        \n",
    "        # PCA n_components 不受限制\n",
    "        return PCA(n_components=ld_n).fit_transform(df.values)\n",
    "        \n",
    "    \n",
    "    return df, label\n",
    "\n",
    "d = dict([('low', 0), ('medium', 1), ('high', 2)])\n",
    "def map_salary(s):\n",
    "    return d.get(s, 0)\n",
    "\n",
    "# 训练集 验证集 测试集\n",
    "def hr_modeling(features, label):\n",
    "    # 引入切分训练集和测试集的函数\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    # 先取值\n",
    "    f_v = features.values\n",
    "    l_v = label.values\n",
    "    # test_size 测试集占多少比例  \n",
    "    # X_tt  训练集部分 X_validation标注\n",
    "    # y_tt  验证集部分 Y_validation标注\n",
    "    # 得到验证集\n",
    "    X_tt, X_validation, Y_tt, Y_validation = train_test_split(f_v, l_v, test_size=0.2)\n",
    "    # 区分验证集和测试集\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_tt, Y_tt, test_size=0.25)\n",
    "    # print(len(X_train), len(X_validation), len(X_test))\n",
    "    \n",
    "    # 衡量指标\n",
    "    # accuracy_score 准确率\n",
    "    # recall_score 召回率\n",
    "    # f1_score F值\n",
    "    from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "    # KNN\n",
    "    # NearestNeighbors 此方法可以直接获得一个点附近最近的几个点\n",
    "    from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\n",
    "    models = []\n",
    "    # n_neigbors 获取最近的几个点\n",
    "    # radius 半径\n",
    "    # algorithm:{'auto', 'ball_tree', 'kd_tree', 'brute'} 指定索引方法\n",
    "    # kd_tree 针对方格子进行的索引  ball_tree 针对球体进行的索引  brute 遍历\n",
    "    # p 就是闵可夫斯基距离公式中的 p 默认为2 \n",
    "    # n_jobs 并行计算的单元数量\n",
    "    knn_clf = KNeighborsClassifier(n_neighbors=3)\n",
    "    knn_clf.fit(X_train, Y_train)\n",
    "        \n",
    "    # 利用训练集进行预测\n",
    "    print(\"Train\")\n",
    "    Y_pred = knn_clf.predict(X_train)\n",
    "    print(\"ACC:\", accuracy_score(Y_train, Y_pred))\n",
    "    print(\"REC:\", recall_score(Y_train, Y_pred))\n",
    "    print(\"F-Score:\", f1_score(Y_train, Y_pred))\n",
    "    \n",
    "    print(\"Validation\")\n",
    "    # 预测 验证集 得到一个预测值\n",
    "    Y_pred = knn_clf.predict(X_validation)\n",
    "    # 拿预测值与实际值进行比较\n",
    "    print(\"ACC:\", accuracy_score(Y_validation, Y_pred))\n",
    "    print(\"REC:\", recall_score(Y_validation, Y_pred))\n",
    "    print(\"F-Score:\", f1_score(Y_validation, Y_pred))\n",
    "    \n",
    "    # 利用测试集进行对比\n",
    "    print(\"Test\")\n",
    "    Y_pred = knn_clf.predict(X_test)\n",
    "    print(\"ACC_test:\", accuracy_score(Y_test, Y_pred))\n",
    "    print(\"REC_test:\", recall_score(Y_test, Y_pred))\n",
    "    print(\"F-Score_test:\", f1_score(Y_test, Y_pred))\n",
    "    \n",
    "    # 将模型进行存储\n",
    "    from sklearn.externals import joblib\n",
    "    # 保存模型\n",
    "    # joblib.dump(knn_clf, \"knn_clf\")\n",
    "    # 使用模型\n",
    "    knn_clf = joblib.load(\"knn_clf\")\n",
    "    print(\"Test_2\")\n",
    "    Y_pred = knn_clf.predict(X_test)\n",
    "    print(\"ACC_test:\", accuracy_score(Y_test, Y_pred))\n",
    "    print(\"REC_test:\", recall_score(Y_test, Y_pred))\n",
    "    print(\"F-Score_test:\", f1_score(Y_test, Y_pred))\n",
    "    \n",
    "def main():\n",
    "    # print(hr_preprocessing(lower_d = False, ld_n = 3))\n",
    "    features, label = hr_preprocessing()\n",
    "    hr_modeling(features, label)\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 朴素贝叶斯 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 概率：P(A)\n",
    "# 条件概率：P(A|B) 条件B发生的条件下，条件A发生的概率\n",
    "# 联合概率：P(A, B) 条件A,B同时发生的概率 \n",
    "# P(A, B) = P(A|B)P(B) = P(B|A)P(A)\n",
    "# 全概率公式：B的概率等于：A事件发生的各种情况下，B事件发生的概率的和\n",
    "# 拉普拉斯平滑：若存在0值，就将所有的数据都加上1，可以规避样本数量等于0的情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "KNN -ACC: 0.9742193577064118\n",
      "KNN -REC: 0.9581993569131833\n",
      "KNN -F-Score: 0.9473206176203451\n",
      "1\n",
      "KNN -ACC: 0.953\n",
      "KNN -REC: 0.9184247538677919\n",
      "KNN -F-Score: 0.902557014512785\n",
      "2\n",
      "KNN -ACC: 0.9546666666666667\n",
      "KNN -REC: 0.9326500732064422\n",
      "KNN -F-Score: 0.9035460992907802\n",
      "0\n",
      "GaussianNB: -ACC: 0.7960884542726969\n",
      "GaussianNB: -REC: 0.7322002756086358\n",
      "GaussianNB: -F-Score: 0.6346804698387418\n",
      "1\n",
      "GaussianNB: -ACC: 0.7796666666666666\n",
      "GaussianNB: -REC: 0.7285513361462729\n",
      "GaussianNB: -F-Score: 0.610489098408957\n",
      "2\n",
      "GaussianNB: -ACC: 0.7986666666666666\n",
      "GaussianNB: -REC: 0.7423133235724744\n",
      "GaussianNB: -F-Score: 0.626699629171817\n",
      "0\n",
      "BernoulliNB: -ACC: 0.8409823313701522\n",
      "BernoulliNB: -REC: 0.4694533762057878\n",
      "BernoulliNB: -F-Score: 0.5882014388489208\n",
      "1\n",
      "BernoulliNB: -ACC: 0.8373333333333334\n",
      "BernoulliNB: -REC: 0.46272855133614627\n",
      "BernoulliNB: -F-Score: 0.5741710296684118\n",
      "2\n",
      "BernoulliNB: -ACC: 0.8476666666666667\n",
      "BernoulliNB: -REC: 0.47584187408491946\n",
      "BernoulliNB: -F-Score: 0.5871725383920505\n",
      "0\n",
      "DecisionTreeGini -ACC: 1.0\n",
      "DecisionTreeGini -REC: 1.0\n",
      "DecisionTreeGini -F-Score: 1.0\n",
      "1\n",
      "DecisionTreeGini -ACC: 0.9746666666666667\n",
      "DecisionTreeGini -REC: 0.9690576652601969\n",
      "DecisionTreeGini -F-Score: 0.9477303988995873\n",
      "2\n",
      "DecisionTreeGini -ACC: 0.9836666666666667\n",
      "DecisionTreeGini -REC: 0.9795021961932651\n",
      "DecisionTreeGini -F-Score: 0.9646719538572458\n",
      "0\n",
      "DecisionTreeEntropy -ACC: 1.0\n",
      "DecisionTreeEntropy -REC: 1.0\n",
      "DecisionTreeEntropy -F-Score: 1.0\n",
      "1\n",
      "DecisionTreeEntropy -ACC: 0.9743333333333334\n",
      "DecisionTreeEntropy -REC: 0.9690576652601969\n",
      "DecisionTreeEntropy -F-Score: 0.9470790378006873\n",
      "2\n",
      "DecisionTreeEntropy -ACC: 0.983\n",
      "DecisionTreeEntropy -REC: 0.9795021961932651\n",
      "DecisionTreeEntropy -F-Score: 0.9632829373650109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "SVM Classifier -ACC: 0.9603289254361596\n",
      "SVM Classifier -REC: 0.9076711070280202\n",
      "SVM Classifier -F-Score: 0.9171501508470643\n",
      "1\n",
      "SVM Classifier -ACC: 0.9566666666666667\n",
      "SVM Classifier -REC: 0.9057665260196905\n",
      "SVM Classifier -F-Score: 0.9083215796897037\n",
      "2\n",
      "SVM Classifier -ACC: 0.961\n",
      "SVM Classifier -REC: 0.9297218155197657\n",
      "SVM Classifier -F-Score: 0.9156452775775054\n",
      "0\n",
      "RandomForest -ACC: 0.9984442715857318\n",
      "RandomForest -REC: 0.994487827285255\n",
      "RandomForest -F-Score: 0.9967771639042357\n",
      "1\n",
      "RandomForest -ACC: 0.986\n",
      "RandomForest -REC: 0.9648382559774965\n",
      "RandomForest -F-Score: 0.9702970297029703\n",
      "2\n",
      "RandomForest -ACC: 0.9913333333333333\n",
      "RandomForest -REC: 0.9736456808199122\n",
      "RandomForest -F-Score: 0.9808259587020649\n",
      "0\n",
      "AdaBoost -ACC: 0.9604400488943216\n",
      "AdaBoost -REC: 0.9131832797427653\n",
      "AdaBoost -F-Score: 0.917820867959372\n",
      "1\n",
      "AdaBoost -ACC: 0.9563333333333334\n",
      "AdaBoost -REC: 0.9043600562587905\n",
      "AdaBoost -F-Score: 0.9075511644318985\n",
      "2\n",
      "AdaBoost -ACC: 0.9666666666666667\n",
      "AdaBoost -REC: 0.9282576866764275\n",
      "AdaBoost -F-Score: 0.9269005847953217\n"
     ]
    }
   ],
   "source": [
    "# 归一化将数据转化到 0-1 之间的范围\n",
    "# sl:satisfaction_level\n",
    "# le:last_evaluation\n",
    "# npr:number_project\n",
    "# amh:average_monthly_hours\n",
    "# tsc:time_spend_company\n",
    "# wa:Work_accident\n",
    "# pl5:promotion_last_5years\n",
    "# False:MinMaxScaler\n",
    "# True:StandardScaler\n",
    "# 将离散值数值化\n",
    "# dp:department \n",
    "# slr:salary\n",
    "# False:LabelEncoding\n",
    "# True:OneHotEncoder\n",
    "# 降维\n",
    "# lower_d = False  不降维\n",
    "# ld_n = 1 下降的维度\n",
    "def hr_preprocessing(sl=False, le=False, npr=False, amh=False, tsc=False, wa=False, pl5=False, dp=False, slr=False, lower_d = False, ld_n = 1):\n",
    "    df = pd.read_csv('./data/HR.csv')\n",
    "    # 1.清洗数据\n",
    "    df = df.dropna(subset=['satisfaction_level', 'last_evaluation'])\n",
    "    df = df[df['satisfaction_level']<=1][df['salary']!='nme']\n",
    "    # 2.得到标注\n",
    "    label = df[\"left\"]\n",
    "    df = df.drop('left', axis=1)\n",
    "    # 3.特征选择\n",
    "    \n",
    "    # 4.特征处理\n",
    "    scaler_lst = [sl, le, npr, amh, tsc, wa, pl5]\n",
    "    column_lst = ['satisfaction_level', 'last_evaluation', 'number_project', 'average_monthly_hours', 'time_spend_company', 'Work_accident', 'promotion_last_5years']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "        else:\n",
    "            df[column_lst[i]]=StandardScaler().fit_transform(df[column_lst[i]].values.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "    # # 将离散值数值化\n",
    "    scaler_lst = [dp, slr]\n",
    "    column_lst = [\"department\", \"salary\"]\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            if column_lst[i] == \"salary\":\n",
    "                df[column_lst[i]] = [map_salary(s) for s in df['salary'].values]\n",
    "            else:\n",
    "                df[column_lst[i]] = LabelEncoder().fit_transform(df[column_lst[i]])\n",
    "            df[column_lst[i]] = MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "        else:\n",
    "            # OneHotEncoder\n",
    "            df = pd.get_dummies(df, columns=[column_lst[i]])\n",
    "    # 降维\n",
    "    if lower_d:\n",
    "        # n_components 不能大于类的个数\n",
    "        # return LinearDiscriminantAnalysis(n_components=ld_n)\n",
    "        # PCA n_components 不受限制\n",
    "        return PCA(n_components=ld_n).fit_transform(df.values)  \n",
    "    return df, label\n",
    "\n",
    "d = dict([('low', 0), ('medium', 1), ('high', 2)])\n",
    "def map_salary(s):\n",
    "    return d.get(s, 0)\n",
    "\n",
    "# 训练集 验证集 测试集\n",
    "def hr_modeling(features, label):\n",
    "    # 引入切分训练集和测试集的函数\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    # 先取值\n",
    "    f_v = features.values\n",
    "    # 特征的名称\n",
    "    f_names = features.columns.values\n",
    "    # 特征的标签\n",
    "    l_v = label.values\n",
    "    # test_size 测试集占多少比例  \n",
    "    # X_tt  训练集部分 X_validation标注\n",
    "    # y_tt  验证集部分 Y_validation标注\n",
    "    # 得到验证集\n",
    "    X_tt, X_validation, Y_tt, Y_validation = train_test_split(f_v, l_v, test_size=0.2)\n",
    "    # 区分验证集和测试集\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_tt, Y_tt, test_size=0.25)\n",
    "    # print(len(X_train), len(X_validation), len(X_test))\n",
    "    \n",
    "    # 衡量指标\n",
    "    # accuracy_score 准确率\n",
    "    # recall_score 召回率\n",
    "    # f1_score F值\n",
    "    from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "    # KNN\n",
    "    # NearestNeighbors 此方法可以直接获得一个点附近最近的几个点\n",
    "    from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\n",
    "    # 朴素贝叶斯 （适用与离散型的数值）\n",
    "    # GaussianNB 高斯朴素贝叶斯 （特征是高斯分布的）\n",
    "    # BernoulliNB伯努利朴素贝叶斯 （数值是离散的，二值，0和1）若是连续值，也需要先将数值离散化\n",
    "    from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "    # 决策树\n",
    "    from sklearn.tree import DecisionTreeClassifier,export_graphviz\n",
    "    from sklearn.externals.six import StringIO\n",
    "    # 引入SVM  分类器\n",
    "    from sklearn.svm import SVC\n",
    "    # 随机森林\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    # Adaboost\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    \n",
    "    # 图形展示\n",
    "    import pydotplus\n",
    "    \n",
    "    models = []\n",
    "    # n_neigbors 获取最近的几个点\n",
    "    # radius 半径\n",
    "    # algorithm:{'auto', 'ball_tree', 'kd_tree', 'brute'} 指定索引方法\n",
    "    # kd_tree 针对方格子进行的索引  ball_tree 针对球体进行的索引  brute 遍历\n",
    "    # p 就是闵可夫斯基距离公式中的 p 默认为2 \n",
    "    # n_jobs 并行计算的单元数量\n",
    "    models.append((\"KNN\", KNeighborsClassifier(n_neighbors=3)))\n",
    "    # 高斯贝叶斯\n",
    "    models.append((\"GaussianNB:\", GaussianNB()))\n",
    "    # 伯努利朴素贝叶斯\n",
    "    models.append((\"BernoulliNB:\", BernoulliNB()))\n",
    "    # Gini决策树\n",
    "    # min_impurity_split=0.1 最小不纯度的切分\n",
    "#     models.append((\"DecisionTreeGini\",DecisionTreeClassifier(min_impurity_split=0.1)))\n",
    "    models.append((\"DecisionTreeGini\",DecisionTreeClassifier()))\n",
    "    # 信息增益决策树\n",
    "    models.append((\"DecisionTreeEntropy\",DecisionTreeClassifier(criterion=\"entropy\")))\n",
    "    # SVM  分类器\n",
    "    # kernel 核函数 取值 线性 linear、多项式 poly、高斯定向集 rbf、\n",
    "    # degree 如果我们核函数选了多项式，那么我们选择几阶\n",
    "    # max_iter 最大迭代次数\n",
    "    # tol 精度\n",
    "    # decision_function_shape 用于多分类，ovo ovr  默认ovr\n",
    "    # C 如果一个标注分错的话会有多大的惩罚,值越大，就会让更多的点不被错分\n",
    "    models.append((\"SVM Classifier\", SVC(C = 1000)))\n",
    "    # 添加随机森林\n",
    "    # n_estimators 决策树的个数  默认10棵\n",
    "    # criterion 使用的方法  默认是Gini\n",
    "    # max_features 每棵树用了多少特征 默认是auto = sqrt 可选值有 int, float（比例）, auto, sqrt, log2, None(代表全量特征)\n",
    "    # max_depth 最大深度\n",
    "    # min_samples_split 最小样本切分\n",
    "    # min_samples_leaf 一个叶子中的最小样本数\n",
    "    # bootstrap 是否进行有放回的采样 True是  False不放回\n",
    "    # oob_score 在使用放回采样的基础上，使用没有被采集到的样本去评估整体的准确性\n",
    "    # n_jobs 并行数量\n",
    "    models.append((\"RandomForest\", RandomForestClassifier(max_features=None, n_estimators=10)))\n",
    "    # Adaboost\n",
    "    # random_state 随机数的种子值\n",
    "    # base_estimator 弱分类的基本分类器 需要两个属性 classes_ 和 n_classes_ 默认的是决策树\n",
    "    # n_estimators 及联的决策树的数量 默认是50个\n",
    "    # learning_rate 根据0-1的数进行衰减\n",
    "    # algorithm 针对 base_estimator 进行的选择，如果base_estimator可以得到一个判别的概率，那么就可以使用SAMME.R,得到更好的效果\n",
    "    models.append((\"AdaBoost\", AdaBoostClassifier(n_estimators=100)))\n",
    "    \n",
    "    for clf_name, clf in models:\n",
    "        # 先进行拟合 训练\n",
    "        clf.fit(X_train, Y_train)\n",
    "        # 进行预测和评价 【训练集，验证集，测试集】\n",
    "        xy_lst = [(X_train, Y_train), (X_validation, Y_validation), (X_test, Y_test)]\n",
    "        for i in range(len(xy_lst)):\n",
    "            # X_part\n",
    "            X_part = xy_lst[i][0]\n",
    "            # y_part\n",
    "            Y_part = xy_lst[i][1]\n",
    "            # 预测值\n",
    "            Y_pred = clf.predict(X_part)\n",
    "            # 0训练集 1验证集 2测试集\n",
    "            print(i)\n",
    "            print(clf_name, \"-ACC:\", accuracy_score(Y_part,  Y_pred))\n",
    "            print(clf_name, \"-REC:\", recall_score(Y_part,  Y_pred))\n",
    "            print(clf_name, \"-F-Score:\", f1_score(Y_part,  Y_pred))\n",
    "            # 使用 graphviz 将决策树以图形的方式展示出来\n",
    "            # out_file=None 可以直接输出\n",
    "            # feature_names 特征的名称\n",
    "            # class_names 标注的名称\n",
    "            # filled=True 填充\n",
    "            # 方法一：\n",
    "#             dot_data = export_graphviz(clf, out_file=None, \n",
    "#                             feature_names=f_names,\n",
    "#                             class_names=[\"NL\", \"L\"], \n",
    "#                             filled=True,\n",
    "#                             rounded=True,\n",
    "#                             special_characters=True\n",
    "#                            )\n",
    "#             # 绘制图形\n",
    "#             graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "#             # 输出为PDF格式\n",
    "#             graph.write_pdf(\"dt_tree.pdf\")\n",
    "\n",
    "#             # 方法二：\n",
    "#             dot_data = StringIO()\n",
    "#             export_graphviz(clf, out_file=dot_data, \n",
    "#                             feature_names=f_names,\n",
    "#                             class_names=[\"NL\", \"L\"], \n",
    "#                             filled=True,\n",
    "#                             rounded=True,\n",
    "#                             special_characters=True\n",
    "#                            )\n",
    "#             # 绘制图形\n",
    "#             graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "#             # 输出为PDF格式\n",
    "#             graph.write_pdf(\"dt_tree_2.pdf\")\n",
    "\n",
    "    \n",
    "def main():\n",
    "    # print(hr_preprocessing(lower_d = False, ld_n = 3))\n",
    "    features, label = hr_preprocessing()\n",
    "    hr_modeling(features, label)\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成模型与判别模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成模型（对数据要求高，速度快一些）：通过求输入与输出的联合概率分布，再求解类别归类的概率。比如朴素贝叶斯模型（因为其分子是一个联合概率分布）\n",
    "# 判别模型（数据要求不高，速度慢一些，使用的范围更广）：不通过联合概率分布，直接就可以获得输出对应最大分类的概率。比如KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 决策树就是模仿我们做决策的过程，一步步按照特征进行判断。 例如相亲图\n",
    "# 如何决定特征的顺序\n",
    "# 1.信息增益-ID3:回顾熵（是随机变量或整个系统的不确定性，熵越大，随机变量的不确定性就越大）\n",
    "    # 熵变化的大小   每次都选择熵增益大的\n",
    "# 2.信息增益率-C4.5\n",
    "    # 分子是信息增益I(X,Y)，分母是Y的熵H(Y)\n",
    "# 3.Gini系数-CART  也叫不纯度\n",
    "    # 用1减去各个因子的数量除以它的总数再平方，得到一个不纯度。如果一个切分，它的不纯度减小的比较大，我们就可以考虑将这个切分先进行决策。\n",
    "    # 不纯度最低的切分作为当前的切分\n",
    "# ⚠️需要注意的问题\n",
    "# 1.连续值的切分\n",
    "    # 将连续值进行从小到大的排序，然后对每个间隔进行切分，计算切分后的各个因子，取该因子性能最好的连续值切分，作为它的切分。\n",
    "# 2.规则用尽\n",
    "    # 比如有四个特征。有可能四个特征都用完了，但是所剩的数据集合还没有切分干净，这样我们可以采用投票的方式，就是哪个样本多就投哪个，当然也可以接着使用特征进行切分，也就是一个特征可以用多次，进行进一步的切分，最终得到没有杂志的叶子节点。\n",
    "# 3.过拟合\n",
    "    # 如果一个特征可以切割多次，那么最终我们总可以得到一个百分之百的切分方案，但是也会带来过拟合的问题。就需要考虑修枝剪叶。\n",
    "    # 前剪枝：构造决策树之前，我就规定了每个叶子节点最多有多少个样本或者规定了这颗决策树的最大深度。\n",
    "    # 后剪枝：先想尽一切办法构造出一颗决策树，然后对一些样本值比较悬殊的枝叶进行修剪\n",
    "# mac下的 Graphviz 安装\n",
    "# brew install graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代码 （看上面）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM 支持向量机 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比如有两个标注的数据，假如想用一条线将其区分开，会有无数种分法。但只有一种分法可以将两个样本最大程度的区分，也就是说两个标注样本中，分别找出离这条线最近的点，它们离这条线的距离是一样的，并且距离和是最大的。这样的切分就是区分度最大的切分方法。此时离这条线最近的两个标注中的样本，就是SVM中的支持向量。\n",
    "# 多维空间中的面也叫做超平面\n",
    "# 高维面：wTx + b = 0  所得到的面就是分界面，所以一定会有一种标注的样本将值带入，一定大于0，另一种标注的样本值带入，一定都小于0.考虑到多数情况下，样本点不会落在直线上，而与这条直线会有一段距离，所以我们可以假设这段距离。\n",
    "# 分界面: wTxp + b >= e  wTxn + b <= -e \n",
    "# 可简化为：wTxp + b >= 1  wTxn + b <= -1\n",
    "# 遇到复杂情况有两种解决情况\n",
    "# 1.容忍一部分错误情况\n",
    "# 2.扩维：先计算（低维空间），再扩维（利用核函数扩维）\n",
    "# 常用的核函数\n",
    "    # 1.线性核函数：在线性可分的情况下使用\n",
    "    # 2.多项式核函数：二维数据d取2就可以扩充到5维\n",
    "    # 3.高斯颈向基（RBF）核函数：可以将空间映射为无限维\n",
    "# 相对于决策树，SVM的边界更加平滑\n",
    "# ⚠️\n",
    "# 少部分异常：引入松弛变量（在原来的变量中再加入一个变量），为了达到更大的平衡，可容忍少量的错分点。\n",
    "# 样本不平衡：看场景定方案\n",
    "# 多分类：\n",
    "    # one-other：有几个分类就建几个SVM，一个样本过来，我们把每个SVM都过一遍，找成功分类并且离超平面距离最远的一个作为正确分类。\n",
    "    # one-one：分类的两两之间分别进行SVM，比如有4个分类，就会有6个SVM。等样本过来的时候，两两间进行比较进行分类，取出一个被分类次数最多的那个类作为我们的SVM分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代码演示 （看上面）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 集成方法-随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回顾：熵增益的决策树适合解决离散值比较多的问题。而基于Gini系数-CART 决策树适合解决连续值分类的问题。如果标注在空间内，隔离性比较好，KNN、SVM较合适。\n",
    "# 分类器就是一种算法。\n",
    "# 集成学习：就是组合多个模型，以获得更好的效果。\n",
    "# 强可学习：多项式学习算法的效果较为明显\n",
    "# 弱可学习：多项式学习算法的效果不是很明显\n",
    "# 模型的集成方法就可以看做是将多个弱可学习分类器，集合成一个强可学习的分类器的过程。\n",
    "# 集成思想：\n",
    "    # 1.袋装法（bagging）：使用训练集同时训练出几个独立的模型，而在进行预测和判断时，我们分别让被训练出的几个子模型去进行判断，然后对于分类问题我们让它们去投票，它们投票选出最多的结果就是最终判断的结果。\n",
    "    # 袋装法的典型应用：随机森林（需要注意的地方：树的个数，树的特征数（<50即可采用全特征）。\n",
    "    # 树的训练集：一般都是模型训练集的一个子集，如何选择子集：我们可以把选择子集的数量定为和输入的模型的数量是一致的，只是对其进行有放回的采样，大概率的有些样本被采集2次以上，有些一次都没被采集到。通过样本构成了树的差异性。）\n",
    "    # 随机森林使用决策树的好处：1.每个决策树可以并不使用全部的特征。这样减少了模型的规模和计算的复杂度，也可以得到不错的效果。2.不需要剪枝，即可有效避免过拟合 \n",
    "    # 2.提升法（）："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代码实现 看上面"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 集成方法-提升法（boost）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提升法：把子模型串联起来，一个模型以另一个模型的结果为基础进行训练和预测，然后多个模型及联，最终将每个模型的结果进行加权求和得到判决结果\n",
    "# Adaboost 例子\n",
    "    # 优点：\n",
    "    # 精度高，且灵活可调\n",
    "    # 几乎不用担心过拟合\n",
    "    # 简化特征工程流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代码实现 （看上面）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
